"""
Demo pipeline to tie together the desired APIs
"""

import os
import pickle
from langchain.vectorstores import FAISS
from langchain.chains import AnalyzeDocumentChain
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PDFMinerLoader

# if data/intermediate/test_pdf.pkl exists, load it
# into memory as 'data', otherwise load the PDF
# via lanchain
if os.path.exists("data/intermediate/test_pdf.pkl"):
    with open("data/intermediate/test_pdf.pkl", "rb") as f:
        data = pickle.load(f)
else:
    loader = PDFMinerLoader("data/raw/AR2022EN.pdf")
    data = loader.load()
    # save data to a picle file in the data/intermediate folder
    with open("data/intermediate/test_pdf.pkl", "wb") as f:
        pickle.dump(data, f)

# Split the PDF into chunks: You can use a splitter from the LangChain library
# to divide the PDF into smaller chunks based on your requirements. Here's an
# example of splitting the PDF into chunks with a maximum of 1000 words each:

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=20,
    length_function=len,
)

chunks = text_splitter.split_documents(data)

# Save the chunks: Once the PDF is divided into chunks, you can save them to
# individual files:

if not os.path.exists("data/intermediate/test_pdf"):
    os.makedirs("data/intermediate/test_pdf")

for i, chunk in enumerate(chunks):
    with open(f"data/intermediate/test_pdf/chunk_{i}.txt", "w") as f:
        f.write(chunk.page_content)

# Optional: Embedding and similarity search: If you need to perform similarity
# searches among the chunks, you can use LangChain's FAISS vector store along
# with OpenAIEmbeddings python.langchain.com:


faiss_index = FAISS.from_documents(chunks, OpenAIEmbeddings())
docs = faiss_index.similarity_search("Your search query", k=2)

for doc in docs:
    print(doc.page_content)

# Optional: Summarize using GPT and LangChain: If you want to generate a
# summary of the long PDF, you can use the AnalyzeDocumentChain class with
# chain_type='map_reduce' and process the extracted text.


chain = AnalyzeDocumentChain(chain_type="map_reduce")
summary = chain.run(data)
print(summary)

# This pipeline allows you to parse a long PDF, split it into chunks, save the
# chunks to individual files, and optionally perform similarity searches or
# generate a summary using GPT and LangChain. Note that the quality of the
# summary may not be as exciting as that generated by ChatGPT with an
# appropriate prompt, but it can still provide a useful starting point for
# understanding the document's content
